{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc5a7b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-27T08:05:08.217062Z",
     "start_time": "2023-08-27T08:05:08.173209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "普遍宽度: 548 像素, 普遍高度: 822 像素\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f666e88e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:39:55.159960Z",
     "start_time": "2023-08-06T01:39:53.820618Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721ab74f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:39:55.174961Z",
     "start_time": "2023-08-06T01:39:55.160960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define other missing variables\n",
    "nz = 100  # Size of the latent vector\n",
    "ngf = 64  # Size of feature maps in the generator\n",
    "ndf = 64  # Size of feature maps in the discriminator\n",
    "nc = 3    # Number of channels (RGB images)\n",
    "image_size = 64  # Size of the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c313b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:39:55.190961Z",
     "start_time": "2023-08-06T01:39:55.175960Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义生成器（Generator）\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.nz = nz  # Add the 'nz' attribute\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() # 输出在[-1, 1]范围内的像素值\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "\n",
    "# 定义判别器（Discriminator）\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False), # 输出一个标量值，表示输入图像的真实性\n",
    "            nn.Sigmoid() # 输出范围在0-1之间，表示输入图像为真实图像的概率\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab5e0b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:39:55.206960Z",
     "start_time": "2023-08-06T01:39:55.191961Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义GAN模型部分\n",
    "class GAN:\n",
    "    def __init__(self, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion):\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.discriminator_loss = []  # To track discriminator loss during training\n",
    "\n",
    "    def train(self, data_loader, num_epochs, log_interval=100, save_img_interval=500):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator.to(device)\n",
    "        self.discriminator.to(device)\n",
    "\n",
    "        fixed_noise = torch.randn(64, self.generator.nz, 1, 1, device=device)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, real_images in enumerate(data_loader):\n",
    "                real_images = real_images.to(device)\n",
    "                batch_size = real_images.size(0)\n",
    "\n",
    "                # 训练判别器（Discriminator）\n",
    "                self.discriminator_optimizer.zero_grad()\n",
    "                # 生成假图片\n",
    "                z = torch.randn(batch_size, self.generator.nz, 1, 1, device=device)\n",
    "                fake_images = self.generator(z)\n",
    "                # 计算判别器损失，注意需要detach()假图片，避免梯度传递到生成器\n",
    "                real_preds = self.discriminator(real_images).view(-1)\n",
    "                fake_preds = self.discriminator(fake_images.detach()).view(-1)\n",
    "                d_loss = 0.5 * (torch.mean((real_preds - 1) ** 2) + torch.mean(fake_preds ** 2))\n",
    "                d_loss.backward()\n",
    "                self.discriminator_optimizer.step()\n",
    "\n",
    "                # 训练生成器（Generator）\n",
    "                self.generator_optimizer.zero_grad()\n",
    "                # 生成假图片并计算判别器输出\n",
    "                # Make sure the generator input (z) is also on the same device\n",
    "                z = torch.randn(batch_size, self.generator.nz, 1, 1, device=device)\n",
    "                fake_images = self.generator(z)\n",
    "                fake_preds = self.discriminator(fake_images).view(-1)\n",
    "                # 计算生成器损失\n",
    "                g_loss = 0.5 * torch.mean((fake_preds - 1) ** 2)\n",
    "                g_loss.backward()\n",
    "                self.generator_optimizer.step()\n",
    "\n",
    "                # Track discriminator loss\n",
    "                self.discriminator_loss.append(d_loss.item())\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    # Log generated images to TensorBoard\n",
    "                    with torch.no_grad():\n",
    "                        fake_images = self.generator(fixed_noise).detach().cpu()\n",
    "                    fake_images_grid = vutils.make_grid(fake_images, padding=2, normalize=True)\n",
    "                    self.writer.add_image(\"Generated Images\", fake_images_grid, global_step=epoch * len(data_loader) + batch_idx)\n",
    "\n",
    "                if batch_idx % save_img_interval == 0:\n",
    "                    # Save a batch of generated images to a folder\n",
    "                    with torch.no_grad():\n",
    "                        fake_images_batch = self.generator(fixed_noise).detach().cpu()\n",
    "                    os.makedirs(f\"data/multiple_stages_images\", exist_ok=True)\n",
    "                    for i in range(fake_images_batch.size(0)):\n",
    "                        vutils.save_image(fake_images_batch[i], f\"data/multiple_stages_images/fake_epoch_{epoch}_batch_{batch_idx}_img_{i}.png\")\n",
    "                        \n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8197a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:39:55.222960Z",
     "start_time": "2023-08-06T01:39:55.208961Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据加载和预处理\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_root, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.image_files = [f for f in os.listdir(data_root) if f.lower().endswith(\".tif\")]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_root, self.image_files[idx])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            # 返回数据（不返回标签）\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_name}: {e}\")\n",
    "            # 返回一个空tensor，表示数据加载失败\n",
    "            return torch.empty((3, 64, 64))\n",
    "\n",
    "def get_data_loader(data_root, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomDataset(data_root, transform=transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70cf9af6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:39:55.238961Z",
     "start_time": "2023-08-06T01:39:55.223961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global variable to track the best discriminator loss and the corresponding generator\n",
    "best_discriminator_loss = float('inf')\n",
    "best_generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a134334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:39:55.254960Z",
     "start_time": "2023-08-06T01:39:55.239961Z"
    }
   },
   "outputs": [],
   "source": [
    "# 主函数\n",
    "def objective(trial):\n",
    "    global best_discriminator_loss\n",
    "    global best_generator\n",
    "\n",
    "    batch_size = 16\n",
    "    num_epochs = 200\n",
    "    lr_g = trial.suggest_float(\"lr_g\", 1e-5, 1e-2, log=True)\n",
    "    lr_d = trial.suggest_float(\"lr_d\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_loader = get_data_loader(\"data/Plaster_side\", batch_size)\n",
    "\n",
    "    nz = 100  # Size of the latent vector\n",
    "\n",
    "    generator = Generator(nz, ngf, nc).to(device)\n",
    "    discriminator = Discriminator(nc, ndf).to(device)\n",
    "\n",
    "    generator_optimizer = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # 定义GAN实例并添加TensorBoard writer\n",
    "    gan = GAN(generator, discriminator, generator_optimizer, discriminator_optimizer, criterion)\n",
    "    gan.writer = SummaryWriter(log_dir=f\"runs/{trial.number}\")\n",
    "\n",
    "    gan.train(data_loader, num_epochs)\n",
    "\n",
    "    # Calculate the average discriminator loss as the optimization objective\n",
    "    avg_discriminator_loss = sum(gan.discriminator_loss) / len(gan.discriminator_loss)\n",
    "\n",
    "    # Save the best generator's images if the current model performs better\n",
    "    if avg_discriminator_loss < best_discriminator_loss:\n",
    "        best_discriminator_loss = avg_discriminator_loss\n",
    "        best_generator = gan.generator\n",
    "\n",
    "    return avg_discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d236a669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:39:55.270960Z",
     "start_time": "2023-08-06T01:39:55.255961Z"
    }
   },
   "outputs": [],
   "source": [
    "# 可视化训练过程\n",
    "def visualize_training(study):\n",
    "    # 使用Tensorboard可视化训练过程\n",
    "    writer = SummaryWriter(log_dir=\"runs\")\n",
    "\n",
    "    # Get the trials as a DataFrame\n",
    "    df = study.trials_dataframe()\n",
    "\n",
    "    # If the 'params' column is not present, use a fallback for hyperparameters\n",
    "    if 'params' not in df.columns:\n",
    "        for i in range(len(df)):\n",
    "            value = df.loc[i, \"value\"]\n",
    "            writer.add_scalar(\"Loss/Discriminator\", value, i)\n",
    "    else:\n",
    "        for i in range(len(df)):\n",
    "            value = df.loc[i, \"value\"]\n",
    "            params = df.loc[i, \"params\"]\n",
    "\n",
    "            writer.add_scalar(\"Loss/Discriminator\", value, i)\n",
    "            for name, param in params.items():\n",
    "                writer.add_scalar(\"Parameters/\" + name, param, i)\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d319a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T01:18:08.508523Z",
     "start_time": "2023-08-28T01:18:01.897596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d5349bccfd5930c5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d5349bccfd5930c5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'runs'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
